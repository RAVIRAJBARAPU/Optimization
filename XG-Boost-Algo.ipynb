{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config(config_write_path):\n",
    "    \"\"\"\n",
    "    config_write_path : folder path to write config file\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    config['id_columns'] = ['BatchNo', 'starttime', 'season', 'Month', 'endtime', 'BrandID', 'BatchID']\n",
    "    \n",
    "    config['imp_columns'] = ['AC BASE DUSTING MTR CURRENT', 'CG1 MTR TT', 'CG2 AHU MTR CURRENT', 'CG2 AHU RH', 'CG2 AHU TT',\n",
    "                             'CG2 DC MTR CURRENT', 'CG2 MTR PT', 'CG2 MTR TT', 'COURSE SODA SP', 'DC FINES SP', 'DC FINES WT',\n",
    "                             'FBC INLET TT', 'FBC1 BLW PT', 'FBC1 SF10202A CURRENT', 'FBC1 SF10202B CURRENT',\n",
    "                             'HLAS HOT TANK TEMP', 'MT 3RD FLOOR', 'MT BH', 'NI DOSING TANK TEMP', 'PADDELMIXER HLAS SP',\n",
    "                             'PADDELMIXER NI SP', 'PADDELMIXER RV SP', 'POSTMIX AC BASE SP', 'POSTMIX CLAY SP',\n",
    "                             'POSTMIXING CURRENT', 'PREMIX AC BASE SP', 'PREMIX SODA SP', 'PREMIX SULPHATE SP', 'RM FINES SP',\n",
    "                             'RV BASE DOSING TANK TEMP', 'TT 3RD FLOOR', 'TT BH', 'ST GRINDER BRG TT1', 'ST GRINDER BRG TT2',\n",
    "                             'ST GRINDER BRG TT3', 'ST GRINDER BRG TT4']\n",
    "    \n",
    "    config['train_data_split'] = [0.7]\n",
    "    config['daily_model_list'] = []\n",
    "    write_config(config, os.path.join(config_write_path, 'config.txt'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_config(data_dict, config_file_path):\n",
    "    def cross(dd):\n",
    "        key_list = list(set(list(dd.keys())) - set(['id_columns', 'imp_columns', 'train_data_split']))\n",
    "        for i in key_list:\n",
    "            l = dd[i]\n",
    "            if len(l)==0:\n",
    "                continue;\n",
    "            if len(l)==1:\n",
    "                try:\n",
    "                    dd[i] = [os.path.abspath(l[0])]\n",
    "                except:\n",
    "                    continue;\n",
    "            else:\n",
    "                try:\n",
    "                    dd[i] = [os.path.abspath(l[j]) for j in range(len(l))]\n",
    "                except:\n",
    "                    continue;\n",
    "        return dd\n",
    "    data_dict = cross(data_dict)\n",
    "    t = open(config_file_path, 'w')\n",
    "    for ind, key in enumerate(data_dict):\n",
    "        if ind==0:\n",
    "            t.write(\"{\\n\")\n",
    "            t.write(\"{}:{},\\n\".format(key, data_dict[key]))#str(key), str(data_dict[key])\n",
    "        elif ind==(len(data_dict)-1):\n",
    "            t.write(\"{}:{}\\n\".format(key, data_dict[key]))\n",
    "            t.write(\"}\")\n",
    "        else:\n",
    "            t.write(\"{}:{},\\n\".format(key, data_dict[key]))\n",
    "    t.close()\n",
    "\n",
    "def read_config(config_file_path):\n",
    "    rt = open(config_file_path, 'r')\n",
    "    lines = rt.readlines()\n",
    "    sk = ''.join(map(str, [i.strip() for i in lines]))\n",
    "    config_dict = yaml.load(sk)\n",
    "    rt.close()\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_Agglomeration_Data(excel_folder_path, write_path, config_path):\n",
    "    \"\"\"\n",
    "    excel_folder_path : folder path of xlsx files\n",
    "    write_path      : write file path\n",
    "    config_path : full path of config file (.../.../xxxxx.txt)\n",
    "    \n",
    "    \"\"\"\n",
    "    fil = [i for i in os.listdir(excel_folder_path) if i.endswith('.xlsx')]\n",
    "    if len(fil)==0:\n",
    "        fil = [i for i in os.listdir(excel_folder_path) if i.endswith('.csv')]\n",
    "    \n",
    "    ind = [int(fil[i].split('-')[0]) for i in range(len(fil))]\n",
    "    \n",
    "    files = [l for _,l in sorted(zip(ind, fil))]\n",
    "    \n",
    "    for i in range(len(files)):\n",
    "        if files[0].endswith('.xlsx'):\n",
    "            f = pd.read_excel(os.path.join(excel_folder_path, files[i]), sheet_name='Agglo')\n",
    "        else:\n",
    "            f = pd.read_csv(os.path.join(excel_folder_path, files[i]), sheet_name='Agglo')\n",
    "        \n",
    "        if i==0:\n",
    "            df = pd.DataFrame(columns=list(f.columns))\n",
    "        \n",
    "        df = pd.concat([df, f], ignore_index=True, axis=0)\n",
    "    \n",
    "    name = 'consolidated_Agglomeration_Data2'+ '_' + str(int(time.time())) + '.csv'\n",
    "    df.to_csv(os.path.join(write_path, name))\n",
    "    config = read_config(config_path)\n",
    "    config['consolidated_agglo_data_path'] = [os.path.join(write_path, name)]\n",
    "    write_config(config, config_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def consolidate_centreline_Data(excel_file_path, write_path, config_path):\n",
    "    \"\"\"\n",
    "    excel_file_path : path of excel [/.../..xlsx]\n",
    "    write_path      : write file path\n",
    "    config_path : full path of config file (.../.../xxxxx.txt)\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.concat(pd.read_excel(excel_file_path, sheet_name=None), ignore_index=True)\n",
    "    \n",
    "    name = 'consolidated_centreline_Data2' + '_' + str(int(time.time())) + '.csv'\n",
    "    \n",
    "    df.to_csv(os.path.join(write_path, name))\n",
    "    config = read_config(config_path)\n",
    "    config['consolidated_centreline_data_path'] = [os.path.join(write_path, name)]\n",
    "    write_config(config, config_path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(agglo_table, centreline_table, database_path, config_path, mode='pred'):\n",
    "    \"\"\"\n",
    "    config_path : full path of config file (.../.../xxxxx.txt)\n",
    "    mode        : pred or train\n",
    "    \n",
    "    \"\"\"\n",
    "    config = read_config(config_path)\n",
    "    # if mode=='train':\n",
    "    #     agglo_path = config['consolidated_agglo_data_path'][0]\n",
    "    #     centre_line_path = config['consolidated_centreline_data_path'][0]\n",
    "    #     write_path = os.getcwd()#os.path.dirname(centre_line_path)\n",
    "        \n",
    "    # elif len(config['daily_model_list'])>0:\n",
    "    #     folder_path = os.path.abspath(config['daily_model_list'][-1])\n",
    "    #     agglo_path = os.path.join(folder_path, 'agglo_data', 'agglo.xlsx')\n",
    "    #     centre_line_path = os.path.join(folder_path, 'centreline_data', 'centreline.xlsx')\n",
    "    #     write_path = str(folder_path)\n",
    "    # else:\n",
    "    #     today_date = dt.datetime.today().strftime (\"%Y-%m-%d\")\n",
    "    #     folder_path = os.path.join(os.path.dirname(config['results_params_csv_path'][0]), str(today_date))\n",
    "    #     agglo_path = os.path.join(folder_path, 'agglo_data', 'agglo.xlsx')\n",
    "    #     centre_line_path = os.path.join(folder_path, 'centreline_data', 'centreline.xlsx')\n",
    "    #     write_path = str(folder_path)\n",
    "        \n",
    "    agglo = table_to_df(agglo_table)\n",
    "    centerline = table_to_df(centreline_table)\n",
    "    \n",
    "    \n",
    "    conn = sqlite3.connect(\":memory:\")\n",
    "    agglo['starttime'] = agglo['Date '] +' '+agglo['Time ']\n",
    "    agglo['starttime'] = pd.to_datetime(agglo['starttime'])\n",
    "    agglo['endtime'] = agglo['starttime'] + pd.to_timedelta(agglo['PADDELMIXER RT'], unit='s')\n",
    "    agglo['BatchNo '] = np.arange(1, len(agglo)+1)\n",
    "    centerline['datetime'] = centerline['Date ']+' '+centerline['Time ']\n",
    "    centerline['datetime'] = pd.to_datetime(centerline['datetime'])\n",
    "    centerline = centerline[centerline['FBC1 BLW HZ']>=0]\n",
    "    agglo.to_sql('agglo', con=conn, index = False)\n",
    "    centerline.to_sql('centerline', con=conn, index = False)\n",
    "    \n",
    "    qry = \"select * from centerline as c inner join agglo as a on c.datetime between a.starttime and a.endtime;\"\n",
    "        \n",
    "    df1 = pd.read_sql_query(qry, conn)\n",
    "    \n",
    "    rm_col = ['Unnamed: 0', 'Date ', 'Time ', 'DateAndTime ', 'datetime', 'Date .1', 'Time .1', 'Shift ']\n",
    "    \n",
    "    rem_col = [i for i in rm_col if i in list(df1.columns)]\n",
    "    \n",
    "    df1.drop(columns = rem_col, inplace = True)\n",
    "    df = df1.groupby(['BatchNo ','starttime', 'endtime', 'Recipe ', 'BrandID ', 'BatchID ', 'EQID ']).mean().reset_index()\n",
    "    \n",
    "    \n",
    "    # name = 'merged_data' + '_' + str(int(time.time())) + '.csv'\n",
    "    # df.to_csv(os.path.join(write_path, name))\n",
    "    # config['merged_data_path'] = [os.path.join(write_path, name)]\n",
    "    # write_config(config, config_path)\n",
    "    append_df_into_table(agglo, database_path, 'Agglmoration')\n",
    "    append_df_into_table(centerline, database_path, 'Centerline')\n",
    "    append_df_into_table(df, database_path, 'Merged_Data')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(database_path, config_path, mode='train'):\n",
    "    \"\"\"\n",
    "    config_path : full path of config file (.../.../xxxxx.txt)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    config = read_config(config_path)\n",
    "    \n",
    "    # merge_data_path = config['merged_data_path'][0]\n",
    "    # df = pd.read_csv(merge_data_path)\n",
    "    # write_path = os.path.dirname(merge_data_path)\n",
    "    table_name = 'Merged_Data'\n",
    "    if mode=='train':\n",
    "        df = read_df_table_from_database(database_path, table_name)\n",
    "    else:\n",
    "        d = read_df_table_from_database(database_path, table_name)\n",
    "        kd = d.iloc[-1,:]\n",
    "        df = pd.DataFrame(np.expand_dims(np.array(kd), 0), columns=list(kd.keys()))\n",
    "        \n",
    "    df.columns = [c.strip() for c in df.columns.values.tolist()]\n",
    "    spcols = [col for col in df.columns if 'SP' in col]\n",
    "\n",
    "    uwcols = ['DC FINES SP','RM FINES SP']\n",
    "    spcols = [e for e in spcols if e not in uwcols]\n",
    "    Totalsp= df[spcols].sum(axis=1)\n",
    "    df = df.assign(Totalsp = Totalsp.values)\n",
    "\n",
    "    df = df.drop(df[(df['BrandID'] == 'ARIEL') & (df['POSTMIX CLAY SP'] > 0.0)].index)\n",
    "\n",
    "    df = df[df['Totalsp'] == 1050]\n",
    "    \n",
    "    col = df.columns\n",
    "    col1 = [a for a in col if '.1' not in a and '.2' not in a]\n",
    "    df = df.loc[:, col1]\n",
    "    if mode=='train':\n",
    "        df = df.iloc[1:, :]\n",
    "\n",
    "    df.dropna(how = 'all', axis = 1, inplace=True)\n",
    "\n",
    "    df['starttime'] = pd.to_datetime(df['starttime'])\n",
    "\n",
    "    df['Month'] = df['starttime'].dt.month\n",
    "\n",
    "    df['season'] = ['summer' if a==3 or a==4 or a==5 or a==6 else 'monsoon' if a==7 or a==8 or a==9 else 'post_monsoon' if a==10 or a==11 else 'winter' for a in df.Month.values]\n",
    "    \n",
    "    id_cols = config['id_columns']\n",
    "\n",
    "    imp_cols = config['imp_columns']\n",
    "    \n",
    "    cols = id_cols+imp_cols\n",
    "    df.columns = df.columns.str.rstrip()\n",
    "    df = df.loc[:,cols]\n",
    "\n",
    "    for c in imp_cols:\n",
    "        df[c] = df[c].astype(float)\n",
    "\n",
    "    df.dropna(how = 'any', axis = 0, inplace=True)\n",
    "\n",
    "    # name = 'preprocessed_data' + '_' +str(int(time.time())) + '.csv'\n",
    "    # df.to_csv(os.path.join(write_path, name))\n",
    "    # config['preprocessed_data_path'] = [os.path.join(write_path, name)]\n",
    "    # write_config(config, config_path)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_method(df, train_data_split, imp_cols, id_cols):\n",
    "    col_ck = 'Month'\n",
    "    df.sort_values(by=[col_ck], inplace=True)\n",
    "    season_list = df[col_ck].unique().tolist()\n",
    "    \n",
    "    split = {'train': [], 'test': []}\n",
    "    for a in season_list:\n",
    "        temp = df[df[col_ck]==a]\n",
    "#         temp = shuffle(temp, random_state=0)\n",
    "        train_n = int(len(temp)*train_data_split)\n",
    "        split['train'].append(temp.iloc[0:train_n, :])\n",
    "        split['test'].append(temp.iloc[train_n:, :])\n",
    "    \n",
    "    train_df = pd.concat(split['train'])\n",
    "    test_df = pd.concat(split['test'])\n",
    "    \n",
    "    train_cols = list(set(imp_cols) - set([\"POSTMIX AC BASE SP\"]))\n",
    "    X_train = train_df.loc[:, train_cols]\n",
    "    X_test = test_df.loc[:, train_cols]\n",
    "    \n",
    "    id_train = train_df.loc[:, id_cols]\n",
    "    id_test = test_df.loc[:, id_cols]\n",
    "    \n",
    "    y_train = train_df[\"POSTMIX AC BASE SP\"].values\n",
    "    y_test = test_df[\"POSTMIX AC BASE SP\"].values\n",
    "    return X_train, X_test, y_train, y_test, id_train, id_test\n",
    "\n",
    "def x_y_scale(df, imp_cols):\n",
    "    x_col = list(set(imp_cols) - set([\"POSTMIX AC BASE SP\"]))\n",
    "\n",
    "    sc_x = preprocessing.StandardScaler().fit(df.loc[:, x_col])\n",
    "    df.loc[:, x_col] = sc_x.transform(df.loc[:, x_col])\n",
    "\n",
    "    sc_y = preprocessing.StandardScaler().fit(df.loc[:, [\"POSTMIX AC BASE SP\"]])\n",
    "    df.loc[:, [\"POSTMIX AC BASE SP\"]] = sc_y.transform(df.loc[:, [\"POSTMIX AC BASE SP\"]])\n",
    "    \n",
    "    return df, sc_x, sc_y\n",
    "\n",
    "\n",
    "def append_df_into_table(df, database_path, table_name):\n",
    "    \n",
    "    conn = sqlite3.connect(database_path)\n",
    "    c = conn.cursor()\n",
    "    # try:\n",
    "    #     c.execute('CREATE TABLE {} {}'.format(str(table_name), tuple(df.columns)))#Brand text, Price number\n",
    "    #     conn.commit()\n",
    "    # except:\n",
    "    #     pass\n",
    "    \n",
    "    df.to_sql(str(table_name), conn, if_exists='append', index = False)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def table_to_df(table):\n",
    "    conn = sqlite3.connect(\":memory:\")\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT * FROM {}'.format(table))\n",
    "    cols=[i[0] for i in c.description]\n",
    "    d = pd.DataFrame(c.fetchall(), columns=cols)\n",
    "    return d\n",
    "\n",
    "def write_table_from_df(df, database_path, table_name):\n",
    "    \n",
    "    conn = sqlite3.connect(database_path)\n",
    "    c = conn.cursor()\n",
    "    try:\n",
    "        c.execute('CREATE TABLE {} {}'.format(str(table_name), tuple(df.columns)))#Brand text, Price number\n",
    "        conn.commit()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df.to_sql(str(table_name), conn, if_exists='replace', index = False)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def read_df_table_from_database(database_path, table_name):\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT * FROM {}'.format(table_name))\n",
    "    cols=[i[0] for i in c.description]\n",
    "    d = pd.DataFrame(c.fetchall(), columns=cols)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost_with_grid_search(df, brand, database_path, config_path, model_write_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    df    : preprocessed df\n",
    "    brand : brand name of the product\n",
    "    config_path : full path of config file (.../.../xxxxx.txt)\n",
    "    model_write_path : path where to save the model weights\n",
    "    \n",
    "    \"\"\"\n",
    "    config = read_config(config_path)\n",
    "    \n",
    "    # preprocess_data_path = config['preprocessed_data_path'][0]\n",
    "    # df = pd.read_csv(preprocess_data_path)\n",
    "    \n",
    "    df = df[df['BrandID']==brand]\n",
    "    \n",
    "    id_cols = config['id_columns']\n",
    "\n",
    "    imp_cols = config['imp_columns']\n",
    "    \n",
    "    train_data_split = float(config['train_data_split'][0])\n",
    "    \n",
    "    cols = id_cols+imp_cols\n",
    "    df.columns = df.columns.str.rstrip()\n",
    "    df = df.loc[:,cols]\n",
    "    \n",
    "    df, sc_x, sc_y = x_y_scale(df, imp_cols)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, id_train, id_test = old_method(df, train_data_split, imp_cols, id_cols)\n",
    "    \n",
    "    regressor = xgb.XGBRegressor()\n",
    "    \n",
    "    param_dist = {'n_estimators': list(np.arange(150,1000,100)),\n",
    "                  'learning_rate': list(np.arange(0.01, 0.5, 0.05)),\n",
    "                  'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "                  'reg_lambda':[1],\n",
    "                  'gamma':[0]\n",
    "                 }\n",
    "    \n",
    "    reg = GridSearchCV(estimator=regressor,  \n",
    "                       scoring=make_scorer(r2_score),\n",
    "                        param_grid= param_dist,\n",
    "                        verbose=False)\n",
    "    \n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = reg.predict(X_test)\n",
    "    \n",
    "    MSE_test = mean_squared_error(y_test, y_pred)\n",
    "    R2_test = r2_score(y_test, y_pred)\n",
    "    score = reg.score(X_test, y_test)\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    \n",
    "    y_pred_tr = reg.predict(X_train)\n",
    "    MSE_tr = mean_squared_error(y_train, y_pred_tr)\n",
    "    R2_tr = r2_score(y_train, y_pred_tr)\n",
    "    score_tr = reg.score(X_train, y_train)\n",
    "    RMSE_tr = np.sqrt(MSE_tr)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(model_write_path):\n",
    "        os.makedirs(model_write_path)\n",
    "    \n",
    "    model_name = 'model_'+ str(brand) + '_' + str(time.strftime(\"%Y-%m-%d\")) +'.pickle'\n",
    "    \n",
    "    model_file_path = os.path.join(model_write_path, model_name)\n",
    "    \n",
    "    f = open(model_file_path, 'wb')\n",
    "    pickle.dump(reg, f)\n",
    "    f.close()\n",
    "    \n",
    "    X_train.loc[:,:] = sc_x.inverse_transform(X_train.loc[:,:])\n",
    "    y_train = sc_y.inverse_transform(y_train)\n",
    "    y_pred_tr = sc_y.inverse_transform(y_pred_tr)\n",
    "    X_train.loc[:, \"POSTMIX AC BASE SP\"] = y_train\n",
    "    X_train.loc[:, \"PRED POSTMIX AC BASE SP\"] = y_pred_tr\n",
    "    X_train = pd.concat([X_train, id_train], axis=1)\n",
    "    \n",
    "    Features = list(X_test.columns)\n",
    "    Target_variable = 'POSTMIX AC BASE SP'\n",
    "    \n",
    "    X_test.loc[:,:] = sc_x.inverse_transform(X_test.loc[:,:])\n",
    "    y_test = sc_y.inverse_transform(y_test)\n",
    "    y_pred = sc_y.inverse_transform(y_pred)\n",
    "    X_test.loc[:, \"POSTMIX AC BASE SP\"] = y_test\n",
    "    X_test.loc[:, \"PRED POSTMIX AC BASE SP\"] = y_pred\n",
    "    X_test = pd.concat([X_test, id_test], axis=1)\n",
    "    \n",
    "    scaler_name = 'sc_y_'+str(brand) + '_' + str(time.strftime(\"%Y-%m-%d\")) + '.pkl'\n",
    "    sc_path = os.path.join(model_write_path, scaler_name)\n",
    "    pickle.dump(sc_y,open(sc_path,'wb'))\n",
    "    config['gs_sc_y_'+str(brand)+'_path'] = [sc_path]\n",
    "    \n",
    "    scaler_x_name = 'sc_x_'+str(brand) + '_' + str(time.strftime(\"%Y-%m-%d\")) + '.pkl'\n",
    "    sc_x_path = os.path.join(model_write_path, scaler_x_name)\n",
    "    pickle.dump(sc_x,open(sc_x_path,'wb'))\n",
    "    config['gs_sc_x_'+str(brand)+'_path'] = [sc_x_path]\n",
    "    \n",
    "    best = reg.best_params_\n",
    "    result_path = os.path.join(model_write_path, 'results_params.csv')\n",
    "\n",
    "    if not os.path.exists(result_path):\n",
    "        res = pd.DataFrame(columns=['Date_of_Run', 'BrandID', 'MSE_Train', 'R2_Train',\n",
    "                                    'MSE_Test', 'R2_Test', 'Gamma', 'Learning_Rate', 'Max_Depth', 'n_estimators', 'Lambda'])\n",
    "    else:\n",
    "        res = pd.read_csv(result_path)\n",
    "\n",
    "    res_data = np.array([time.strftime(\"%Y-%m-%d %H:%M:%S\"), brand, MSE_tr, R2_tr, MSE_test, R2_test,\n",
    "                best['gamma'], best['learning_rate'], best['max_depth'], best['n_estimators'], best['reg_lambda']])\n",
    "\n",
    "    temp = pd.DataFrame(np.expand_dims(res_data, 0), columns=['Date_of_Run', 'BrandID', 'MSE_Train', 'R2_Train','MSE_Test','R2_Test',\n",
    "                                                              'Gamma', 'Learning_Rate', 'Max_Depth', 'n_estimators', 'Lambda'])\n",
    "\n",
    "    res = pd.concat([res, temp], axis=0, ignore_index=True)\n",
    "    \n",
    "    res.loc[0, 'Model_file_location'] = str(model_file_path)\n",
    "    res.loc[0, 'SC_y_file_location'] = str(sc_path)\n",
    "    res.loc[0, 'SC_x_file_location'] = str(sc_x_path)\n",
    "    \n",
    "    mod_data = np.expand_dims(np.array([brand, str(Features), Target_variable, str(id_cols)]), 0)\n",
    "    model_df = pd.DataFrame(mod_data, columns=['Brand','Features','Target_variable', 'Id_columns'])\n",
    "\n",
    "    \n",
    "    res = append_df_into_table(res, database_path, 'Model_FineTuning_Log')\n",
    "    \n",
    "    model_df = append_df_into_table(model_df, database_path, 'Model')\n",
    "    \n",
    "    # res.to_csv(result_path)\n",
    "    \n",
    "    # total_df = pd.concat([X_train, X_test], axis=0)\n",
    "    \n",
    "    # total_df.to_csv(os.path.join(model_write_path, 'full_data_prediction.csv'))\n",
    "    \n",
    "    config['gs_model_'+str(brand)+'_path']= [model_file_path]\n",
    "    # config['results_params_csv_path'] = [result_path]\n",
    "    # config['full_data_predicton_path'] = [os.path.join(model_write_path, 'full_data_prediction.csv')]\n",
    "    \n",
    "    write_config(config, config_path)\n",
    "    \n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost(df, brand, database_path, config_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    brand : brand name of the product\n",
    "    config_path : full path of config file (.../.../xxxxx.txt)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    config = read_config(config_path)\n",
    "    \n",
    "    # preprocess_data_path = config['preprocessed_data_path'][0]\n",
    "    # df = pd.read_csv(preprocess_data_path)\n",
    "    \n",
    "    df = df[df['BrandID']==brand]\n",
    "    \n",
    "    id_cols = config['id_columns']\n",
    "\n",
    "    imp_cols = config['imp_columns']\n",
    "    \n",
    "    train_data_split = float(config['train_data_split'][0])\n",
    "    \n",
    "    cols = id_cols+imp_cols\n",
    "    df.columns = df.columns.str.rstrip()\n",
    "    df = df.loc[:,cols]\n",
    "    \n",
    "    df, sc_x, sc_y = x_y_scale(df, imp_cols)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, id_train, id_test = old_method(df, train_data_split, imp_cols, id_cols)\n",
    "    \n",
    "    # params = pd.read_csv(config['results_params_csv_path'][0])\n",
    "    params = read_df_table_from_database(database_path, 'Model_FineTuning_Log')\n",
    "    \n",
    "    params = params[params.BrandID==brand]\n",
    "    params.sort_values(by=['Date_of_Run'], inplace=True)\n",
    "    \n",
    "    reg = xgb.XGBRegressor()\n",
    "    \n",
    "    reg.gamma = float(params.Gamma.values[-1])\n",
    "\n",
    "    reg.learning_rate = float(params.Learning_Rate.values[-1])\n",
    "\n",
    "    reg.max_depth = int(params.Max_Depth.values[-1])\n",
    "\n",
    "    reg.n_estimators = int(params.n_estimators.values[-1]);\n",
    "\n",
    "    reg.reg_lambda = float(params.Lambda.values[-1])\n",
    "    \n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = reg.predict(X_test)\n",
    "    \n",
    "    MSE_test = mean_squared_error(y_test, y_pred)\n",
    "    R2_test = r2_score(y_test, y_pred)\n",
    "    score = reg.score(X_test, y_test)\n",
    "    RMSE_test = np.sqrt(MSE_test)\n",
    "    \n",
    "    y_pred_tr = reg.predict(X_train)\n",
    "    MSE_tr = mean_squared_error(y_train, y_pred_tr)\n",
    "    R2_tr = r2_score(y_train, y_pred_tr)\n",
    "    score_tr = reg.score(X_train, y_train)\n",
    "    RMSE_tr = np.sqrt(MSE_tr)\n",
    "    \n",
    "    tomo_date = (dt.datetime.today()).strftime (\"%Y-%m-%d\")# + dt.timedelta(days=1)\n",
    "    \n",
    "    model_write_path = os.path.dirname(params.Model_file_location.values[-1])\n",
    "    \n",
    "    daily_model_path = os.path.join(model_write_path, str(tomo_date))\n",
    "    \n",
    "    if not os.path.exists(daily_model_path):\n",
    "        os.makedirs(daily_model_path)\n",
    "    \n",
    "    model_name = 'model_'+ str(brand) + '_' + str(tomo_date) +'.pickle'\n",
    "    \n",
    "    model_file_path = os.path.join(daily_model_path, model_name)\n",
    "    \n",
    "    f = open(model_file_path, 'wb')\n",
    "    pickle.dump(reg, f)\n",
    "    f.close()\n",
    "    \n",
    "#     X_train.loc[:,:] = sc_x.inverse_transform(X_train.loc[:,:])\n",
    "#     y_train = sc_y.inverse_transform(y_train)\n",
    "#     y_pred_tr = sc_y.inverse_transform(y_pred_tr)\n",
    "#     X_train.loc[:, \"POSTMIX AC BASE SP\"] = y_train\n",
    "#     X_train.loc[:, \"PRED POSTMIX AC BASE SP\"] = y_pred_tr\n",
    "#     X_train = pd.concat([X_train, id_train], axis=1)\n",
    "    \n",
    "#     X_test.loc[:,:] = sc_x.inverse_transform(X_test.loc[:,:])\n",
    "#     y_test = sc_y.inverse_transform(y_test)\n",
    "#     y_pred = sc_y.inverse_transform(y_pred)\n",
    "#     X_test.loc[:, \"POSTMIX AC BASE SP\"] = y_test\n",
    "#     X_test.loc[:, \"PRED POSTMIX AC BASE SP\"] = y_pred\n",
    "#     X_test = pd.concat([X_test, id_test], axis=1)\n",
    "    \n",
    "    scaler_name = 'sc_y_'+str(brand) + '_' + str(tomo_date) + '.pkl'\n",
    "    sc_path = os.path.join(daily_model_path, scaler_name)\n",
    "    pickle.dump(sc_y,open(sc_path,'wb'))\n",
    "    \n",
    "    scaler_x_name = 'sc_x_'+str(brand) + '_' + str(tomo_date) + '.pkl'\n",
    "    sc_x_path = os.path.join(daily_model_path, scaler_x_name)\n",
    "    pickle.dump(sc_x,open(sc_x_path,'wb'))\n",
    "    \n",
    "    res_data = np.array([tomo_date, brand, MSE_tr, R2_tr, MSE_test, R2_test,\n",
    "                params.Gamma.values[-1], params.Learning_Rate.values[-1], params.Max_Depth.values[-1],\n",
    "                params.n_estimators.values[-1], params.Lambda.values[-1],\n",
    "                str(model_file_path), str(sc_path), str(sc_x_path)])\n",
    "\n",
    "    temp = pd.DataFrame(np.expand_dims(res_data, 0), columns=['Date_of_Run', 'BrandID', 'MSE_Train', 'R2_Train','MSE_Test','R2_Test',\n",
    "                                                              'Gamma', 'Learning_Rate', 'Max_Depth', 'n_estimators', 'Lambda', 'Model_file_location', 'SC_y_file_location', 'SC_x_file_location'])\n",
    "    \n",
    "#     res_data = np.array([tomo_date, brand, MSE_tr, R2_tr, MSE_test, R2_test,\n",
    "#                 params.Gamma.values[-1], params.Learning_Rate.values[-1], params.Max_Depth.values[-1],\n",
    "#                 params.n_estimators.values[-1], params.Lambda.values[-1],\n",
    "#                 str(model_file_path), str(sc_path)])\n",
    "\n",
    "#     temp = pd.DataFrame(np.expand_dims(res_data, 0), columns=['Date_of_Run', 'BrandID', 'MSE_Train', 'R2_Train','MSE_Test','R2_Test',\n",
    "#                                                               'Gamma', 'Learning_Rate', 'Max_Depth', 'n_estimators', 'Lambda', 'Model_file_location', 'SC_y_file_location'])\n",
    "\n",
    "    # res = pd.DataFrame(columns=['Date_of_Run', 'BrandID', 'MSE_Train', 'R2_Train',\n",
    "    #                             'MSE_Test', 'R2_Test', 'Gamma', 'Learning_Rate', 'Max_Depth',\n",
    "    #                             'n_estimators', 'Lambda', 'Model_file_location', 'SC_y_file_location'])\n",
    "\n",
    "    temp = append_df_into_table(temp, database_path, 'Model_Training_Log')\n",
    "    \n",
    "    # daily_model_list = config['daily_model_list']\n",
    "    # daily_model_list.append(daily_model_path)\n",
    "    # config['daily_model_list'] = daily_model_list\n",
    "    \n",
    "    # total_df = pd.concat([X_train, X_test], axis=0)\n",
    "    \n",
    "    # data_name = 'full_data_prediction_'+ str(tomo_date) + '.csv'\n",
    "    # total_df.to_csv(os.path.join(daily_model_path, data_name))\n",
    "        \n",
    "    write_config(config, config_path)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost_prediction(df, database_path, config_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    df    : preprocessed dataframe\n",
    "    brand : brand name of the product\n",
    "    config_path : full path of config file (.../.../xxxxx.txt)\n",
    "    \n",
    "    \"\"\"\n",
    "    config = read_config(config_path)\n",
    "    \n",
    "    # preprocess_data_path = config['preprocessed_data_path'][0]\n",
    "    # df = pd.read_csv(preprocess_data_path)\n",
    "    \n",
    "    brand = df.BrandID.values[0]\n",
    "    \n",
    "    df = df[df['BrandID']==brand]\n",
    "    \n",
    "    id_cols = config['id_columns']\n",
    "\n",
    "    imp_cols = config['imp_columns']\n",
    "    \n",
    "    train_data_split = float(config['train_data_split'][0])\n",
    "    \n",
    "    cols = id_cols+imp_cols\n",
    "    cols2 = list(set(cols) - set([\"POSTMIX AC BASE SP\"]))\n",
    "    df.columns = df.columns.str.rstrip()\n",
    "    df2 = df.loc[:,cols2]\n",
    "    \n",
    "    train_cols = list(set(imp_cols) - set([\"POSTMIX AC BASE SP\"]))\n",
    "    \n",
    "    # if len(config['daily_model_list'])==0:\n",
    "    #     gs_model_path = config['gs_model_'+str(brand)+'_path']\n",
    "    #     gs_date = os.path.basename(gs_model_path).split('.')[0].split('_')[-1]\n",
    "    #     folder_path = os.path.join(os.path.dirname(config['results_params_csv_path'][0]))\n",
    "    #     model_date = str(gs_date)\n",
    "    #     pred_write_path = os.path.join(folder_path, str(time.strftime(\"%Y-%m-%d\")))\n",
    "    #     if not os.path.exists(pred_write_path):\n",
    "    #         os.makedirs(pred_write_path)\n",
    "    # else:\n",
    "    #     folder_path = os.path.abspath(config['daily_model_list'][-1])\n",
    "    #     model_date = os.path.basename(folder_path)\n",
    "    #     pred_write_path = str(folder_path)\n",
    "    \n",
    "    model_df = read_df_table_from_database(database_path, 'Model_Training_Log')\n",
    "    model_df = model_df[model_df.BrandID==brand]\n",
    "    \n",
    "    if len(model_df)==0:\n",
    "        model_df = read_df_table_from_database(database_path, 'Model_FineTuning_Log')\n",
    "        model_df = model_df[model_df.BrandID==brand]\n",
    "        model_df.sort_values(by=['Date_of_Run'], inplace=True)\n",
    "    else:\n",
    "        model_df.sort_values(by=['Date_of_Run'], inplace=True)\n",
    "    \n",
    "    folder_path =  os.path.dirname(model_df.Model_file_location.values[-1])\n",
    "    model_date = os.path.basename(folder_path)\n",
    "    \n",
    "    # sc_x = preprocessing.StandardScaler().fit(df2.loc[:, train_cols])\n",
    "    # df2.loc[:, train_cols] = sc_x.transform(df2.loc[:, train_cols])\n",
    "    \n",
    "    # x_scaler = 'sc_x_'+str(brand) + '_' + str(model_date) + '.pkl'\n",
    "    # sc_x_path = os.path.join(folder_path, x_scaler)\n",
    "    sc_x_path = model_df.SC_x_file_location.values[-1]\n",
    "    sc_x = pickle.load(open(os.path.abspath(sc_x_path), 'rb'))\n",
    "    \n",
    "    df2.loc[:, train_cols] = sc_x.transform(df2.loc[:, train_cols])\n",
    "    \n",
    "    last_model_name = 'model_'+ str(brand) + '_' + str(model_date) +'.pickle'\n",
    "    last_model_path = os.path.join(folder_path, last_model_name)\n",
    "    \n",
    "    model = pickle.load(open(os.path.abspath(last_model_path), 'rb'))\n",
    "    \n",
    "    y_pred = model.predict(df2.loc[:, train_cols])\n",
    "    \n",
    "    # y_scaler = 'sc_y_'+str(brand) + '_' + str(model_date) + '.pkl'\n",
    "    # sc_y_path = os.path.join(folder_path, y_scaler)\n",
    "    sc_y_path = model_df.SC_y_file_location.values[-1]\n",
    "    sc_y = pickle.load(open(os.path.abspath(sc_y_path), 'rb'))\n",
    "    \n",
    "    ori_y_pred = sc_y.inverse_transform(y_pred)\n",
    "    \n",
    "#     df.loc[:, \"PRED POSTMIX AC BASE SP\"] = ori_y_pred\n",
    "    \n",
    "    pred_data = np.expand_dims(np.array([df.BatchNo.values[0], df.BrandID.values[0], str(ori_y_pred)]), 0)\n",
    "    pred_df = pd.DataFrame(pred_data, columns=['BatchNo', 'BrandID', 'Prediction_value'])\n",
    "    \n",
    "    pred_df = append_df_into_table(pred_df, database_path, 'Prediction')\n",
    "    # pred_name = 'prediction' + '_' + str(time.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    # df.to_csv(os.path.join(pred_write_path, pred_name+'.csv'))\n",
    "    \n",
    "    # config[pred_name] = [os.path.join(pred_write_path, pred_name+'.csv')]\n",
    "    \n",
    "    write_config(config, config_path)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
